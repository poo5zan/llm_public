{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMk/zVUmq302oUqSNDbMcfb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poo5zan/llm_public/blob/main/Extract_Information.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract Information from company's private data using open source Large Language Models. In this document, I have used Llama3 8 billion model, which is an open source model from Meta (Facebook).\n",
        "\n",
        "Important Note: Please make sure that the connected runtime has GPU. By default, the runtime might not have GPU. So, in order to change the runtime, click on the 'Runtime' menu, then click 'Change runtime type', and select 'T4 GPU'. The runtime type should be 'Python 3'."
      ],
      "metadata": {
        "id": "SQRnd-rJv9Sr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ollama Installation"
      ],
      "metadata": {
        "id": "Ru-qbnpxxcPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and Start Ollama\n",
        "# The command to start ollama i.e 'ollama serve' has been started in a new process.\n",
        "# If you just run the command !ollama serve, then it will run the process in the main UI thread,\n",
        "# thus blocking everything. You can try that too, and then revert back to this process method\n",
        "!curl https://ollama.ai/install.sh | sh\n",
        "import subprocess\n",
        "process_serve = subprocess.Popen(\"ollama serve\", shell=True)"
      ],
      "metadata": {
        "id": "M84Sfxr-xYPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Drive\n",
        "\n",
        "Any files uploaded to the colab runtime will be deleted with each new session, and uploading the documents again and again is tedious. Thus, I prefer using google drive to store the documents. The following command will open a new window to connect to the google drive."
      ],
      "metadata": {
        "id": "rlhcM9lSyMDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# connect google drive and mount the root folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fWLITugiv-jT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "368436be-3991-48fa-c902-287b53c6262f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I created a folder named \"llm_data\" in my google drive as the root folder. You can name it anything.\n",
        "\n",
        "TIP: After you utilize all the compute resources for free tier, then you can try with new gmail account. In case of google drive, you need to share this llm_data folder with the new gmail account, and then add shortcut to this folder in the root folder i.e 'My Drive' of the new google account. This way you don't need to re-upload your documents."
      ],
      "metadata": {
        "id": "cPEEHIJ7zF2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "llm_data_folder = \"/content/drive/My Drive/llm_data\"\n",
        "print('llm_data_folder ', llm_data_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwLEaQlmzBd4",
        "outputId": "6b1e1c39-d1c5-4c12-8dc3-aac13e4b4a08"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llm_data_folder  /content/drive/My Drive/llm_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Python Dependencies\n",
        "\n",
        "During the installation of the dependencies. You might receive pip dependency error and it might ask you to restart the session. So what do you do ? Simply restart the session with the 'Restart Session' button at the end of the error message.\n",
        "\n",
        "After this, run the command once again. Be happy if there are no errors."
      ],
      "metadata": {
        "id": "MYMaiULg0Jn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index llama-parse ollama llama-index-llms-ollama  llama-index-llms-huggingface \\\n",
        "llama-index-embeddings-huggingface llama-index-extractors-entity transformers torch accelerate bitsandbytes joblib"
      ],
      "metadata": {
        "id": "VNzmcXC60Mt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this document, we will be using llama3 both from Ollama and HuggingFace. The llama3 in HuggingFace is not public, thus you need to request access the owner of the model in the HuggingFace. It's a simple process, create an account in HuggingFace and request access to this model, https://huggingface.co/meta-llama/Meta-Llama-3-8B\n",
        "\n",
        "In the following command, we login to the huggingface cli. This part is optional if you are only going to use llama3 from Ollama.\n",
        "\n",
        "Create your personal access token in the profile settings of huggingface portal, https://huggingface.co/settings/tokens\n"
      ],
      "metadata": {
        "id": "5TOytIjK0o3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Huggingface cli\n",
        "# uncomment the following for the login\n",
        "# !huggingface-cli login --token your-huggingface-access-token-goes-here"
      ],
      "metadata": {
        "id": "ZDOHoV_E0nsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "w1VYZk1K2p9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define some config classes\n",
        "from enum import Enum\n",
        "\n",
        "class ModelSource(Enum):\n",
        "    Default = 0\n",
        "    Ollama = 1\n",
        "    HuggingFace = 2\n",
        "\n",
        "class QuantizationBit(Enum):\n",
        "    Default = 0\n",
        "    Four = 4\n",
        "    Eight = 8\n",
        "\n",
        "class OllamaConfig():\n",
        "    def __init__(self, model_name: str, request_timeout: int):\n",
        "        self.model_name = model_name\n",
        "        self.request_timeout = request_timeout\n",
        "\n",
        "class HuggingFaceConfig():\n",
        "    def __init__(self, model_name:str, quantize_model: bool, quantization_bit: QuantizationBit):\n",
        "        self.model_name = model_name\n",
        "        self.quantize_model = quantize_model\n",
        "        self.quantization_bit = quantization_bit\n",
        "\n",
        "class ModelConfig():\n",
        "    def __init__(self, model_source: ModelSource,\n",
        "                 ollama_config: OllamaConfig,\n",
        "                 huggingface_config: HuggingFaceConfig):\n",
        "        self.model_source = model_source\n",
        "        self.ollama_config = ollama_config\n",
        "        self.huggingface_config = huggingface_config\n"
      ],
      "metadata": {
        "id": "8P0G-CVE2rvB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.ollama import Ollama\n",
        "import json\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import ollama\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from ollama import Client\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "class ExtractInformation():\n",
        "    def __init__(self, model_config: ModelConfig):\n",
        "        self.model_config = model_config\n",
        "\n",
        "    def get_llm_model(self):\n",
        "        if self.model_config.model_source == ModelSource.Ollama:\n",
        "            print(f'Using Ollama model:{self.model_config.ollama_config.model_name}')\n",
        "            return Ollama(model = self.model_config.ollama_config.model_name,\n",
        "                          request_timeout = self.model_config.ollama_config.request_timeout,\n",
        "                          json_mode=True)\n",
        "\n",
        "        elif self.model_config.model_source == ModelSource.HuggingFace:\n",
        "            quantization_config = BitsAndBytesConfig()\n",
        "            if self.model_config.huggingface_config.quantize_model:\n",
        "                if self.model_config.huggingface_config.quantization_bit == QuantizationBit.Four:\n",
        "                    quantization_config = BitsAndBytesConfig(\n",
        "                        load_in_4bit=True,\n",
        "                        bnb_4bit_compute_dtype=torch.float16,\n",
        "                        bnb_4bit_quant_type=\"nf4\",\n",
        "                        bnb_4bit_use_double_quant=True,\n",
        "                    )\n",
        "                elif self.model_config.huggingface_config.quantization_bit == QuantizationBit.Eight:\n",
        "                    quantization_config = BitsAndBytesConfig(\n",
        "                        load_in_8bit=True,\n",
        "                        llm_int8_enable_fp32_cpu_offload=True\n",
        "                    )\n",
        "                else:\n",
        "                    raise ValueError(f\"Invalid Huggingface quantization bit \\\n",
        "                     {self.model_config.huggingface_config.quantization_bit}\")\n",
        "\n",
        "            print(f'Using Huggingface model:{self.model_config.huggingface_config.model_name},  \\\n",
        "                quantize:{self.model_config.huggingface_config.quantize_model}, \\\n",
        "                quantization bit:{self.model_config.huggingface_config.quantization_bit}')\n",
        "            return HuggingFaceLLM(\n",
        "                model_name = self.model_config.huggingface_config.model_name,\n",
        "                tokenizer_name = self.model_config.huggingface_config.model_name,\n",
        "                context_window=4096,\n",
        "                max_new_tokens=256,\n",
        "                model_kwargs={\"quantization_config\": quantization_config},\n",
        "                tokenizer_kwargs={\"max_length\": 4096},\n",
        "                generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
        "                device_map=\"auto\"\n",
        "            )\n",
        "\n",
        "\n",
        "    def call_llm(self, query_engine, query_text: str):\n",
        "        try :\n",
        "            return query_engine.query(query_text)\n",
        "        except Exception as ex:\n",
        "            if self.model_config.model_source == ModelSource.Ollama and \"Client error '404 Not Found'\" in str(ex):\n",
        "                print(\"LLM not found. So pull llm  model: \", self.model_config.ollama_config.model_name)\n",
        "                # since the ollama in installed in the localhost,\n",
        "                # the following code to pull the model works well,\n",
        "                # However, if the ollama is installed somewhere else,\n",
        "                # then uncomment the code below\n",
        "                ollama.pull(self.model_config.ollama_config.model_name)\n",
        "\n",
        "                # from ollama import Client\n",
        "                # client = Client(host=ollama_url_goes_here)\n",
        "                # client.pull(self.model_config.ollama_config.model_name)\n",
        "                print(\"LLM pull completed. Thus, retry llm call.\")\n",
        "                return self.call_llm(query_engine, query_text)\n",
        "            else:\n",
        "                print(\"Exception in calling llm \", ex)\n",
        "                raise ex\n",
        "\n",
        "    def find_value_from_document(self, query_engine, query_text: str):\n",
        "        query_result = self.call_llm(query_engine, query_text)\n",
        "        if not query_result:\n",
        "            raise ValueError('No response from llm call')\n",
        "        query_response = ''\n",
        "        if query_result.response:\n",
        "            query_response = query_result.response\n",
        "\n",
        "        return {\n",
        "            'query_text': query_text,\n",
        "            'response': query_response\n",
        "        }\n",
        "\n",
        "    def extract(self, input_data_folder: str, query_text: str):\n",
        "        Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "        Settings.llm = self.get_llm_model()\n",
        "\n",
        "        documents = SimpleDirectoryReader(input_data_folder).load_data()\n",
        "        vector_index = VectorStoreIndex.from_documents(documents)\n",
        "        query_engine = vector_index.as_query_engine()\n",
        "\n",
        "        return self.find_value_from_document(query_engine, query_text)\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "0tHcVwTb39ki"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "continue_execution = True\n",
        "ollama_config = OllamaConfig(\"llama3\", 60)\n",
        "model_config = ModelConfig(ModelSource.Ollama, ollama_config, None)\n",
        "extract_information = ExtractInformation(model_config)\n",
        "while continue_execution:\n",
        "    input_text = input(\"Enter your query (Enter exit to exit the program): \")\n",
        "    if input_text == \"exit\":\n",
        "        continue_execution = False\n",
        "    else:\n",
        "        print(\"Query: \", input_text)\n",
        "        response = extract_information.extract(llm_data_folder, input_text)\n",
        "        print('Response:', response['response'])\n",
        ""
      ],
      "metadata": {
        "id": "xGZYNwJIC4z3",
        "outputId": "c15d2c6f-4752-440c-98f4-45228856a7fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query (Enter exit to exit the program): What is interesting point here\n",
            "Query:  What is interesting point here\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Ollama model:llama3\n",
            "Response: {\"It was as weird as it sounds.\" : \"I resumed all my old patterns, except now there were doors where there hadn't been. Now when I was tired of walking, all I had to do was raise my hand, and (unless it was raining) a taxi would stop to pick me up. Now when I walked past charming little restaurants I could go in and order lunch.\"}\n",
            "Enter your query (Enter exit to exit the program): exit\n"
          ]
        }
      ]
    }
  ]
}